{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('ted_en-20160408.xml'):\n",
    "    urllib.request.urlretrieve(\"https://github.com/oxford-cs-deepnlp-2017/practical-1/blob/master/ted_en-20160408.xml?raw=true\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = lxml.etree.parse('ted_en-20160408.xml')\n",
    "input_text = doc.xpath('//content/text()')\n",
    "label = doc.xpath('//head/keywords/text()')\n",
    "del doc\n",
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess sentences to exclude all characters except alphabets and numbers\n",
    "texts = [re.sub(r'\\([^)]*\\)', '',text) for text in input_text]\n",
    "texts = [re.sub('r([^a-zA-Z0-9\\s])',' ',text) for text in texts] #Included '.'\n",
    "texts = [re.sub('[^a-zA-Z0-9\\']',' ',text) for text in texts] #To replace '.' with ' '\n",
    "texts = [re.sub('[^a-zA-Z0-9 ]','',text) for text in texts]\n",
    "texts = [text.lower() for text in texts] #uppercase->lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   thank you   thank you very much  like the speaker before me    i am a ted virgin  i guess  im also the first time here  and      i dont know what to say   im'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2069][:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text greater than 500 words are: 2076\n"
     ]
    }
   ],
   "source": [
    "text_labels = zip(texts,label)\n",
    "texts = [text_label for text_label in text_labels if len(text_label[0]) > 500]\n",
    "print('number of text greater than 500 words are:',len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts,labels = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here are two reasons companies fail  they only do more of the same  or they only do whats new  to me the real  real solution to quality growth is figuring out the balance between two activities  exploration and exploitation  both are necessary  but it can be too much of a good thing  consider facit  im actually old enough to remember them  facit was a fantastic company  they were born deep in the swedish forest  and they made the best mechanical calculators in the world  everybody used them  and what did facit do when the electronic calculator came along  they continued doing exactly the same  in six months  they went from maximum revenue     and they were gone  gone  to me  the irony about the facit story is hearing about the facit engineers  who had bought cheap  small electronic calculators in japan that they used to double check their calculators   facit did too much exploitation  but exploration can go wild  too  a few years back  i worked closely alongside a european biotech company  lets call them oncosearch  the company was brilliant  they had applications that promised to diagnose  even cure  certain forms of blood cance  every day was about creating something new  they were extremely innovative  and the mantra was   when we only get it right   or even   we want it perfect   the sad thing is  before they became perfect    even good enough    they became obsolete  oncosearch did too much exploration  i first heard about exploration and exploitation about 15 years ago  when i worked as a visiting scholar at stanford university  the founder of the idea is jim march  and to me the power of the idea is its practicality  exploration  exploration is about coming up with whats new  its about search  its about discovery  its about new products  its about new innovations  its about changing our frontiers  our heroes are people who have done exploration  madame curie  picasso  neil armstrong  sir edmund hillary  etc  i come from norway  all our heroes are explorers  and they deserve to be  we all know that exploration is risky  we dont know the answers  we dont know if were going to find them  and we know that the risks are high  exploitation is the opposite  exploitation is taking the knowledge we have and making good  bette  exploitation is about making our trains run on time  its about making good products faster and cheape  exploitation is not risky    in the short term  but if we only exploit  its very risky in the long term  and i think we all have memories of the famous pop groups who keep singing the same songs again and again  until they become obsolete or even pathetic  thats the risk of exploitation  so if we take a long term perspective  we explore  if we take a short term perspective  we exploit  small children  they explore all day  all day its about exploration  as we grow olde  we explore less because we have more knowledge to exploit on  the same goes for companies  companies become  by nature  less innovative as they become more competent  and this is  of course  a big worry to ceos  and i hear very often questions phrased in different ways  for example   how can i both effectively run and reinvent my company   o   how can i make sure that our company changes before we become obsolete or are hit by a crisis   so  doing one well is difficult  doing both well as the same time is art    pushing both exploration and exploitation  so one thing weve found is only about two percent of companies are able to effectively explore and exploit at the same time  in parallel  but when they do  the payoffs are huge  so we have lots of great examples  we have nestl  creating nespresso  we have lego going into animated films  toyota creating the hybrids  unilever pushing into sustainability    there are lots of examples  and the benefits are huge  why is balancing so difficult  i think its difficult because there are so many traps that keep us where we are  so ill talk about two  but there are many  so lets talk about the perpetual search trap  we discover something  but we dont have the patience or the persistence to get at it and make it work  so instead of staying with it  we create something new  but the same goes for that  then were in the vicious circle of actually coming up with ideas but being frustrated  oncosearch was a good example  a famous example is  of course  xerox  but we dont only see this in companies  we see this in the public sector as well  we all know that any kind of effective reform of education  research  health care  even defense  takes 10  15  maybe 20 years to work  but still  we change much more often  we really dont give them the chance  another trap is the success trap  facit fell into the success trap  they literally held the future in their hands  but they couldnt see it  they were simply so good at making what they loved doing  that they wouldnt change  we are like that  too  when we know something well  its difficult to change  bill gates has said   success is a lousy teache  it seduces us into thinking we cannot fail   thats the challenge with success  so i think there are some lessons  and i think they apply to us  and they apply to our companies  the first lesson is  get ahead of the crisis  and any company thats able to innovate is actually able to also buy an insurance in the future  netflix    they could so easily have been content with earlier generations of distribution  but they always    and i think they will always    keep pushing for the next battle  i see other companies that say   ill win the next innovation cycle  whatever it takes   second one  think in multiple time scales  ill share a chart with you  and i think its a wonderful one  any company we look at  taking a one year perspective and looking at the valuation of the company  innovation typically accounts for only about 30 percent  so when we think one yea  innovation isnt really that important  move ahead  take a 10 year perspective on the same company    suddenly  innovation and ability to renew account for 70 percent  but companies cant choose  they need to fund the journey and lead the long term  third  invite talent  i dont think its possible for any of us to be able to balance exploration and exploitation by ourselves  i think its a team sport  i think we need to allow challenging  i think the mark of a great company is being open to be challenged  and the mark of a good corporate board is to constructively challenge  i think thats also what good parenting is about  last one  be skeptical of success  maybe its useful to think back at the old triumph marches in rome  when the generals  after a big victory  were given their celebration  riding into rome on the carriage  they always had a companion whispering in their ea   remembe  youre only human   so i hope i made the point  balancing exploration and exploitation has a huge payoff  but its difficult  and we need to be conscious  i want to just point out two questions that i think are useful  first question is  looking at your own company  in which areas do you see that the company is at the risk of falling into success traps  of just going on autopilot  and what can you do to challenge  second question is  when did i explore something new last  and what kind of effect did it have on me  is that something i should do more of  in my case  yes  so let me leave you with this  whether youre an explorer by nature or whether you tend to exploit what you already know  dont forget  the beauty is in the balance  thank you  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [words for text in texts for words in text.split()]\n",
    "words_count = Counter(words)\n",
    "words_most_common =[word for word,count in words_count.most_common(100)]\n",
    "words_least_common = [word for word,count in words_count.most_common() if count==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of Token: 1948385\n"
     ]
    }
   ],
   "source": [
    "to_remove = words_most_common + words_least_common\n",
    "words_to_remove = set(to_remove)\n",
    "tokens = [word for word in words if word not in words_to_remove] #will be used during T-SNE\n",
    "print('size of Token:',len(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in text.split() if word not in words_to_remove]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "label_coded = ['ooo']*len(labels)\n",
    "for i,keyword in enumerate(labels):\n",
    "    key = keyword.split(', ')\n",
    "    label = list(label_coded[i])\n",
    "    if 'technology' in key:\n",
    "        label[0] = 'T'\n",
    "    if 'entertainment' in key:\n",
    "        label[1] = 'E'\n",
    "    if 'design' in key:\n",
    "        label[2] = 'D'\n",
    "    else:\n",
    "        pass\n",
    "    label_coded[i] =''.join(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ooo', 1130),\n",
       " ('Too', 389),\n",
       " ('oEo', 169),\n",
       " ('ooD', 158),\n",
       " ('ToD', 137),\n",
       " ('TEo', 36),\n",
       " ('TED', 33),\n",
       " ('oED', 24)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_labels=Counter(label_coded)\n",
    "label_count = [word_count for word_count in count_labels.most_common()]\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hotted = np.zeros(shape=(len(labels),8),dtype='int16')\n",
    "label_lookup = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "label_lookup = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "for i,label in enumerate(label_coded):\n",
    "    one_hotted[i][label_lookup.index(label)] = 1\n",
    "print(one_hotted[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.append('<UNK>')\n",
    "tokens.append('<PAD>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 37328\n"
     ]
    }
   ],
   "source": [
    "print('size of vocabulary:',len(vocab))\n",
    "id2word = dict(enumerate(vocab))\n",
    "word2id = dict((val,key) for (key,val) in id2word.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping Text to fall within length of 500; incase if it is shorter then padd with '<UNK>'\n",
    "length = 500 #sentence length\n",
    "stripped_text = []#np.zeros((len(texts),length)\n",
    "for i,text in enumerate(texts):\n",
    "    inputs = []\n",
    "    if len(text) >= 500:\n",
    "        inputs.extend(text[:500])\n",
    "    else:\n",
    "        extra_length = 500-len(text)\n",
    "        extra = ['<PAD>']*extra_length\n",
    "        word_with_extra = text + extra\n",
    "        inputs.extend(word_with_extra)\n",
    "    stripped_text.append(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076\n"
     ]
    }
   ],
   "source": [
    "stripped_length = len(stripped_text)\n",
    "print(stripped_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,code in enumerate(label_coded):\n",
    "    one_hotted[i][label_lookup.index(code)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "text_ids = []\n",
    "for text in stripped_text:\n",
    "    for word in text:\n",
    "        i = word2id[word]\n",
    "        inputs.append(i)\n",
    "    text_ids.append(inputs)\n",
    "    inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33996, 'even', 'even', 3949)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids[0][100] , id2word[text_ids[0][100]], stripped_text[0][100], word2id['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(text_ids,one_hotted))\n",
    "tr_size = round(0.8*len(data))\n",
    "vl_size = round(0.1*len(data))\n",
    "te_size = tr_size + vl_size\n",
    "n_classes = one_hotted.shape[1]\n",
    "train_Xy , val_Xy , test_Xy = [],[],[]\n",
    "for i in np.arange(n_classes):\n",
    "    j = np.zeros(n_classes)\n",
    "    j[i] = 1\n",
    "    temp = [text_ohe for text_ohe in data if text_ohe[1][i]==j[i]]\n",
    "    temp_len = len(temp)\n",
    "    tr_split = round(temp_len*0.8)\n",
    "    val_split = round(temp_len*0.9)\n",
    "    train_Xy.extend(temp[:tr_split])\n",
    "    val_Xy.extend(temp[tr_split:val_split])\n",
    "    test_Xy.extend(temp[val_split:])\n",
    "random.shuffle(train_Xy)\n",
    "random.shuffle(val_Xy)\n",
    "random.shuffle(test_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.from_numpy(np.array([i[0] for i in train_Xy]))\n",
    "train_labels = [i[1] for i in train_Xy]\n",
    "val_ids = torch.from_numpy(np.array([i[0] for i in val_Xy]))\n",
    "val_labels = [i[1] for i in val_Xy]\n",
    "test_ids = torch.from_numpy(np.array([i[0] for i in test_Xy]))\n",
    "test_labels = [i[1] for i in test_Xy]\n",
    "train_labels = torch.tensor([label.tolist().index(1) for label in train_labels], dtype=torch.long)\n",
    "val_labels = torch.tensor([label.tolist().index(1) for label in val_labels], dtype=torch.long)\n",
    "test_labels = torch.tensor([label.tolist().index(1) for label in test_labels], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TED_Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = ID\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy = TED_Dataset(train_ids,train_labels)\n",
    "val_Xy = TED_Dataset(val_ids,val_labels)\n",
    "test_Xy = TED_Dataset(test_ids,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_Xy, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_Xy, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_Xy, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1660"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEDModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dimension=320):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dimension)\n",
    "        self.hidden_layer= nn.Sequential(\n",
    "            nn.Linear(320,100,bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(100, 64, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(64,8,bias=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        out = self.hidden_layer(x)\n",
    "        predicted_class = torch.argmax(out, dim=1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_model = TEDModel(37328)\n",
    "num_epochs = 10\n",
    "batch_size=64\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate=1e-3\n",
    "optimizer = optim.Adam(ted_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), (batch * batch_size + len(X))\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.082108  [   64/ 1660]\n",
      "loss: 2.013074  [  384/ 1660]\n",
      "loss: 1.837508  [  704/ 1660]\n",
      "loss: 1.751937  [ 1024/ 1660]\n",
      "loss: 1.553387  [ 1344/ 1660]\n",
      "loss: 1.447626  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.553050 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.510647  [   64/ 1660]\n",
      "loss: 1.504274  [  384/ 1660]\n",
      "loss: 1.248576  [  704/ 1660]\n",
      "loss: 1.422259  [ 1024/ 1660]\n",
      "loss: 1.291390  [ 1344/ 1660]\n",
      "loss: 1.418209  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.439560 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.335564  [   64/ 1660]\n",
      "loss: 1.256299  [  384/ 1660]\n",
      "loss: 1.468776  [  704/ 1660]\n",
      "loss: 1.394194  [ 1024/ 1660]\n",
      "loss: 1.418483  [ 1344/ 1660]\n",
      "loss: 1.449643  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.407778 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.355038  [   64/ 1660]\n",
      "loss: 1.335801  [  384/ 1660]\n",
      "loss: 1.411776  [  704/ 1660]\n",
      "loss: 1.334147  [ 1024/ 1660]\n",
      "loss: 1.151738  [ 1344/ 1660]\n",
      "loss: 1.167202  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.360845 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.273434  [   64/ 1660]\n",
      "loss: 1.317816  [  384/ 1660]\n",
      "loss: 1.207754  [  704/ 1660]\n",
      "loss: 1.161895  [ 1024/ 1660]\n",
      "loss: 1.090133  [ 1344/ 1660]\n",
      "loss: 1.122399  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.314336 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.444337  [   64/ 1660]\n",
      "loss: 0.987898  [  384/ 1660]\n",
      "loss: 1.189142  [  704/ 1660]\n",
      "loss: 0.986071  [ 1024/ 1660]\n",
      "loss: 0.863774  [ 1344/ 1660]\n",
      "loss: 1.077940  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.253168 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.006972  [   64/ 1660]\n",
      "loss: 1.059727  [  384/ 1660]\n",
      "loss: 0.985713  [  704/ 1660]\n",
      "loss: 0.949052  [ 1024/ 1660]\n",
      "loss: 1.227433  [ 1344/ 1660]\n",
      "loss: 0.950642  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.271674 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.953193  [   64/ 1660]\n",
      "loss: 0.921420  [  384/ 1660]\n",
      "loss: 1.006723  [  704/ 1660]\n",
      "loss: 0.807354  [ 1024/ 1660]\n",
      "loss: 0.870201  [ 1344/ 1660]\n",
      "loss: 0.876067  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.311479 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.673894  [   64/ 1660]\n",
      "loss: 0.756957  [  384/ 1660]\n",
      "loss: 0.816938  [  704/ 1660]\n",
      "loss: 0.728849  [ 1024/ 1660]\n",
      "loss: 0.857751  [ 1344/ 1660]\n",
      "loss: 0.896286  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.336344 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.676377  [   64/ 1660]\n",
      "loss: 0.610417  [  384/ 1660]\n",
      "loss: 0.624949  [  704/ 1660]\n",
      "loss: 0.732236  [ 1024/ 1660]\n",
      "loss: 0.853905  [ 1344/ 1660]\n",
      "loss: 0.612495  [ 1660/ 1660]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.394342 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, ted_model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, ted_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ted_model.state_dict(), \"model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TEDModel(\n",
       "  (embedding): Embedding(37328, 320)\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.4, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TEDModel(37328)\n",
    "model.load_state_dict(torch.load(\"model2.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
